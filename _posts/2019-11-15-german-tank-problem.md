---
layout: post
title: the German tank problem
tags: [statistics, probability, combinatorics, estimation]
snippet: The simplified German tank problem is as follows. During World War 2, the Germans labeled their tanks with sequential serial numbers. The Allied forces used the serial numbers on a captured subset of the German tanks (data!) to estimate the total number of tanks the Germans had.
author: Cory Simon
---

The German tank problem [1] is intellectually stimulating and makes great conversation at wine bars. The problem has a historical context as well [2].

During World War 2, the Germans inscribed their tanks with sequential serial numbers $1,2,...,n$ when they were manufactured. The total number of tanks $n$ that the Germans had, however, was unknown to the Allied forces-- and of great interest.

The Allies captured a (assumed) random sample of $k$ tanks from the German forces (without replacement, of course) and observed their serial numbers, $x_1,x_2,...,x_k$. The German tank problem is to estimate the total number of tanks $n$ from the observed serial numbers $x_1,x_2,...,x_k$.


{:.centerr}
<figure>
    <img src="/images/german_tank_problem/sample.png" alt="image" style="width: 75%;">
    <figcaption>An outcome of a random sample (without replacement) of German tanks, serial numbers inscribed. Based on this observation, how many tanks do you estimate the Germans have?</figcaption>
</figure>

The number of tanks is of course greater than or equal to the maximum serial number observed: $n \geq \max x_i$. Given the sample above, estimating that the Germans have fewer than 156 tanks would be outrageous. However, estimating that the Germans have 156 tanks seems too conservative; this estimate assumes that we happened to capture the most recently manufactured tank. Intuitively, we should estimate the total number of tanks to be 156 *plus some number* to reflect the small likelihood that we captured the very last tank to come out of the factory (as opposed to not capturing it). How large should this number be?

Let's delve into the math to answer this question, but feel free to skip to `# the unbiased estimator` if you want to know right away.

# the data generating process (dgp)

We are collecting a random sample from a population (the tanks) and making an observation about the members of the sample (the serial numbers). Think of tank capturing as a stochastic process governed by an underlying data generating process (dgp). The data here are the set of serial numbers $$\{x_1,x_2,...,x_k\}$$.

Our stochastic model for tank capturing-- the dgp-- is a uniform random selection without replacement from the set of integers $$\{1,2,...,n\}$$. Because all viable outcomes (sets of observed serial numbers) are equally probable, the probability of any particular viable outcome, given we know $n$, is:

$$P(\{x_1, x_2, ..., x_k\} | n)=\dfrac{1}{\binom{n}{k}}$$

A viable outcome is one where $$x_i \in \{1, 2, ..., n\}$$ for all $$i\in \{1,2,...,k\}$$. So our dgp is governed by the probability distribution above, which is a uniform distribution over the sample space. The denominator is the cardinality of the sample space-- how many ways we can select $k$ integers from $n$ without replacement, where the order in which they were selected does not matter.

# simulating the German tank problem in Julia

We now simulate the dgp in the Julia programming language. For elegance, let's first define a data structure for a tank.

```julia
struct Tank
    serial_no::Int
end

tank = Tank(4) # construct tank with serial number 4.
```

The following function uses the `sample` function in `StatsBase.jl` to simulate tank capture.

```julia
function capture_tanks(nb_tanks_captured::Int, nb_tanks::Int)
    # random selection w./o replacement of integers 1,2,...,nb_tanks
    serial_nos = sample(1:nb_tanks, nb_tanks_captured, replace=false)
    # return array of tanks with these serial numbers
    return [Tank(serial_no) for serial_no in serial_nos]
end

captured_tanks = capture_tanks(5, 300) # produces an array of tanks shown above
```

We'll use this function to simulate tank capture and assess the biasedness of different estimators for the number of tanks.

# what is an estimator?

In general, an *estimator* maps an outcome of a random experiment to an estimate of a parameter that characterizes the dgp. In the German tank problem, an outcome is the set of serial numbers on the tanks, $$\{x_1, x_2, ..., x_k\}$$, and we aim to estimate the total number of tanks, $n$, which fully characterizes the dgp (in addition to $k$). One estimator of $n$ for the German tank problem is the maximum observed serial number $m\equiv \max x_i$. 

We aim to derive an *unbiased* estimator, meaning that, on average, the estimator recovers the true value of the parameter in the dgp. The average here is over the ensemble of possible outcomes generated by the dgp. 

The maximum serial number $m$ is a biased estimator because it is either equal to the true value of $n$ or an underestimate. To show this, I ran 10,000 simulations of our dgp using the `capture_tanks` function with $n=100$ and $k=5$. The plot below shows the distribution of the estimated number of tanks using $m \equiv \max x_i$.

{:.centerr}
<figure>
    <img src="/images/german_tank_problem/max_serial_no.png" alt="image" style="width: 75%;">
    <figcaption>Using the maximum serial number as the estimator of $n$ yields a biased estimator; the average of the estimate over 10,000 simulations is less than the true number of tanks $n$.</figcaption>
</figure>

# persuing an unbiased estimator of $n$

Let us proceed to derive an *un*biased estimator based on the maximum serial number observed, $m$. We will find out what number to add to $m$ to arrive at our unbiased estimate of $n$. 


## the likelihood $Pr(M=m|n)$

Let $M$ be the random variable that is the maximum serial number. Then, the probability that we see a certain max serial number $m$, conditioned on $n$, is:

$$Pr(M=m|n) = \dfrac{\binom{m-1}{k-1}}{\binom{n}{k}}$$

since, as the schematic below illustrates, there are $m-1$ choose $k-1$ distinct sets of $k$ serial numbers where the max number is $m$. All outcomes such that the max serial number is $m$ are independent events, so we can add up the probabilities of each event to arrive at the probability of seeing a max serial number of $m$ conditioned on $n$. This distribution petertains only to $m$ such that $n \geq m\geq k$ since the max serial number cannot be more than the total number of tanks or less than the number of captured tanks.

{:.centerr}
<figure>
    <img src="/images/german_tank_problem/show_formula.png" alt="image" style="width: 75%;">
    <figcaption>The number of outcomes such that the maximum serial number is $m$ is equal to the number of ways we can select the remaining $k-1$ tanks from the $m-1$ tanks preceding the tank with the  max serial number $m$.</figcaption>
</figure>

### a useful identity from the normalization of the probability mass function

Since $n \geq m \geq k$:

$$1= \sum_{m=k}^n Pr(M=m|n) = \sum_{m=k}^n \dfrac{\binom{m-1}{k-1}}{\binom{n}{k}}$$

i.e. the probability that $m$ is (inclusive) between $k$ and $m$ is unity. This gives us an identity that we will use later.

$$\binom{n}{k} = \sum_{m=k}^n \binom{m-1}{k-1}$$

In words, intuitively, the cardinality of the sample space for the German tank problem is equal to the number of outcomes where the max serial number is $k$ plus where it is $k+1$ and so on, up to $n$.

## the expected value of the max serial number, $E(M=m\|n)$

Now let us find the expected value of $M$ over the outcomes of our dgp. 

$$E(M | n) = \displaystyle \sum_{m=k}^n m Pr(M=m|n)$$

We're conditioning on knowing $n$, which seems awkward because the whole point of the German tank problem is that we don't know $n$. Our strategy here is to find $E(M \| n)$, then find what $n$ gives an expected value of $M$ equal to the $m$ we oberved. This gives an unbiased estimator for $n$. Using our earlier expression for $Pr(M=m\|n)$:

$$E(M | n) = \sum_{m=k}^n m \frac{\binom{m-1}{k-1}}{\binom{n}{k}}$$

The sum is over $m$, so we can pull the denominator outside of the sum. Also, we can write this as a sum over combinations by absorbing the $m$ into the factorial and multiplying by one in a fancy way:

$$E(M | n) = \dfrac{1}{\binom{n}{k}} \sum_{m=k}^n \frac{k}{k} m \frac{(m-1)!}{(k-1)!(m-k)!}$$

$$E(M | n) = \dfrac{k}{\binom{n}{k}} \sum_{m=k}^n \frac{m!}{k!(m-k)!}$$

$$E(M | n) = \dfrac{k}{\binom{n}{k}} \sum_{m=k}^n \binom{m}{k}$$

To apply our identity above, we use a change of variables in our sum. Define $\tilde{m}$ and $\tilde{k}$ such that $\tilde{m}-1=m$ and $\tilde{k}-1=k$. Then the sum over $\tilde{m}$ is from $\tilde{k}$ to $n+1$:

$$E(M | n) = \dfrac{k}{\binom{n}{k}} \sum_{\tilde{m}=\tilde{k}}^{n+1} \binom{\tilde{m}-1}{\tilde{k}-1}$$

We can now directly invoke our identity:

$$\sum_{\tilde{m}=\tilde{k}}^{n+1} \binom{\tilde{m}-1}{\tilde{k}-1} = \binom{n+1}{\tilde{k}}=\binom{n+1}{k+1}$$

and simplify the expression for $E(M \| n)$:

$$E(M | n) = \dfrac{k}{\binom{n}{k}} \binom{n+1}{k+1}$$

which, upon expanding the combinations into factorials and canceling terms:

$$E(M | n) = \dfrac{k}{k+1}(n+1)$$

For $k=5$, we get $E(M \| n=100)$ is c.a. 84.2, comparable to our simulation above.

## an unbiased estimator

We obtain an unbiased estimator for $n$, $\hat{n}$, by solving the expression above for $E(M \| n)$ for $n$ and replacing $E(M \| n)$ with our observation for $m=\max x_i$. That is, our estimate for the total number of German tanks is the $n$ such that the expected value of $M$ under the data generating process is equal to our observed $m$. We arrive at:

$$\hat{n} = m + \left(\frac{m}{k} - 1\right)$$

Indeed, the estimate of the total number of tanks is the maximum serial number observed plus some number to reflect how unlikely it is to select the most recently manufactured tank (as opposed to missing it). Intuitively, this number gets larger as $m$ gets larger and as $k$ gets smaller. 

This estimator $\hat{n}$ has a very intuitive interpretation. Imagine placing all of the tanks in a line, from 1 to $n$. Then imagine if the tanks we captured were evenly spaced, i.e. had equal gaps of unobserved tanks between them (including the end). The number that we add to $m$ to arrive at the estimate, $m/k-1$, is the gap between the tanks that would be present if the samples were evenly spaced!

{:.centerr}
<figure>
    <img src="/images/german_tank_problem/seven_tanks.png" alt="image" style="width: 75%;">
    <figcaption>If the $k$ captured tanks are evenly spaced among the $m$ total tanks, then the gap between them is $m/k-1$. Consider $n=7$ and $k=3$ with captured tanks labeled red. </figcaption>
</figure>

The estimator $\hat{n}$ for the total number of tanks is unbiased. To see this, I ran 10,000 simulations of the dgp with $n=100$ and $k=5$ using the `capture_tanks` function, then used $\hat{n}$ above to estimate the number of tanks from the max serial number. The distribution of the estimated number of tanks for this estimator is shown below.

{:.centerr}
<figure>
    <img src="/images/german_tank_problem/max_serial_no_plus_gap_if_evenly_spaced.png" alt="image" style="width: 75%;">
    <figcaption>The estimator $\hat{n}$ above is unbiased since, over many simulations, its average value is the true number of tanks.</figcaption>
</figure>

One can also show that the estimator $\hat{n}$ is *efficient*, meaning that it exhibits the smallest variance among all other unbiased estimators. 

Another desirable property of an estimator is *consistency*. Loosely, this means the distribution of the estimator will tighten in on the true value of $n$ as $k$ increases. As we capture more tanks (increase $k$), we become more and more likely to correctly estimate $n$. In the extreme case $k=n$, we will of course always correctly estimate $n$. Below, I show the distribution of the estimated number of tanks $\hat{n}$ among 10,000 simulations ($n=100$) for several different values of $k$. Indeed, the distribution narrows in on the true number of tanks, suggesting that this estimator is consistent.

{:.centerr}
<figure>
    <img src="/images/german_tank_problem/consistency.png" alt="image" style="width: 75%;">
    <figcaption>Computationally exploring the consistency of $\hat{n}$. The distribution of the estimated number of tanks narrows in on the true number of tanks as we capture more tanks.</figcaption>
</figure>

# confidence intervals

Our estimate $\hat{n}$ is a point estimate. In an interval estimate, we desire to give an interval in which the true number of tanks will fall, with a specified confidence. The idea behind "confidence" is as follows. Say the maximum serial number we observed is $m=100$ with a sample of $k=10$. Also say one proposes that the Germans have 10,000 tanks. It could totally be true that we happened to capture only from within the first 100 tanks of the 10,000; we cannot disprove it from our sample. However, given that $n=10,000$, it is an unlikely outcome that all $k=10$ tanks were sampled from only the first 100 of the 10,000 tanks; it is more likely to have a larger max serial number than 100 if there were 10,000 tanks. This gives us some confidence that the Germans have less than 10,000 tanks.

The box plots below visualize the distribution of $\hat{n}$ over 100,000 simulations for a series of $n$, simulated using the `capture_tanks` function. The blue horizontal line is the point estimate $\hat{n}$ for $k=10$ and $m=100$. Based on the box plots, which break the distributions into quantiles, only 25% of the time would we see $\hat{n}$ below what we observe when $n=135$. Thus, based on our observation $m=100$ with $k=10$ captured tanks, we could reject the null hypothesis that $n=135$ at a level of significance of 25%, since it is unlikely that the serial number would be so low (100) if there were 135 tanks. Therefore, if $m=100$ and $k=10$, we would say that the number of tanks the Germans have is less than 135 with 25% confidence.

{:.centerr}
<figure>
    <img src="/images/german_tank_problem/confidence.png" alt="image" style="width: 75%;">
    <figcaption>A series of box plots shows the distribution of $\hat{n}$ over 100,000 simulations. The triangle symbol shows the mean. The blue horizontal line is $\hat{n}$ for when $k=10$ and $m=100$.</figcaption>
</figure>


# what I'm exploring next
* using the Bayesian framework to estimate the number of tanks
* proove that $\hat{n}$ is the minimum variance unbiased estimator (that it is an efficient estimator)
* learn probabilistic programming in the context of the German tank problem

# references

[1] Goodman LA. Serial number analysis. Journal of the American Statistical Association. 1952

[2] Ruggles R, Brodie H. An empirical approach to economic intelligence in World War II. Journal of the American Statistical Association. 1947

[3] KC Border's Introduction to Probability and Statistics notes http://www.math.caltech.edu/~2016-17/2term/ma003/Notes/Lecture18.pdf
